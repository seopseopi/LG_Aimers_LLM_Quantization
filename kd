import torch
import torch.nn as nn
import torch.nn.functional as F
from datasets import load_dataset, Dataset
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    Trainer, 
    TrainingArguments, 
    DataCollatorWithPadding
)

# 1. ê²½ë¡œ ë° ì„¤ì •
PRUNED_PATH = "./model_pruned"
TEACHER_ID = "LGAI-EXAONE/EXAONE-4.0-1.2B"
MASKS_PATH = "pruning_masks.pt"
OUTPUT_KD = "./model_kd"

# 2. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
print("ğŸ’¡ ëª¨ë¸ ë° ìŠ¤ìŠ¹ ëª¨ë¸ ë¡œë”© ì¤‘...")
tokenizer = AutoTokenizer.from_pretrained(TEACHER_ID)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

student = AutoModelForCausalLM.from_pretrained(PRUNED_PATH, torch_dtype=torch.bfloat16, device_map="auto")
teacher = AutoModelForCausalLM.from_pretrained(TEACHER_ID, torch_dtype=torch.bfloat16, device_map="auto")
masks = torch.load(MASKS_PATH)

# ê°€ì¤‘ì¹˜ 0 ìœ ì§€ í•¨ìˆ˜ (Masking)
def apply_masks(model, masks):
    with torch.no_grad():
        for n, p in model.named_parameters():
            if any(m_name in n for m_name in masks.keys()):
                # pruning_masks.ptì˜ í‚¤ê°’ê³¼ ëª¨ë¸ íŒŒë¼ë¯¸í„° ì´ë¦„ ë§¤ì¹­
                key = next(k for k in masks.keys() if k in n)
                p.data *= masks[key].to(p.device)

# 3. ë°ì´í„°ì…‹ ì¤€ë¹„ (labelsë¥¼ ì—¬ê¸°ì„œ ë„£ì§€ ì•ŠìŠµë‹ˆë‹¤)
def get_kd_dataset(num_samples=8000):
    print(f"ğŸ’¡ {num_samples}ê°œì˜ KDìš© ë°ì´í„°ì…‹ ìƒì„± ì¤‘...")
    ds = load_dataset("LGAI-EXAONE/MANTA-1M", split="train", streaming=True)
    samples = []
    for i, ex in enumerate(ds):
        if i >= num_samples: break
        text = tokenizer.apply_chat_template(ex["conversations"], tokenize=False, add_generation_prompt=True)
        enc = tokenizer(text, truncation=True, max_length=512)
        # labelsë¥¼ ë¹¼ê³  input_idsì™€ attention_maskë§Œ ë„˜ê¹ë‹ˆë‹¤.
        samples.append({
            "input_ids": enc.input_ids, 
            "attention_mask": enc.attention_mask
        })
    return Dataset.from_list(samples)

kd_data = get_kd_dataset(num_samples=8000)
# Collatorê°€ ì´ì œ input_idsì˜ ê¸¸ì´ë¥¼ ë°°ì¹˜ë³„ë¡œ ì˜ ë§ì¶°ì¤„ ê²ë‹ˆë‹¤.
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# 4. Masked KD Trainer ì •ì˜ (ìµœì‹  Transformers ê·œê²© ì ìš©)
class MaskedKDTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        # [ìˆ˜ì •] ê°€ì¤‘ì¹˜ 0 ìœ ì§€ë¥¼ ìœ„í•œ ë§ˆìŠ¤í¬ ì ìš©
        apply_masks(model, masks)
        
        # Student ì¶”ë¡ 
        outputs = model(**inputs)
        
        # Teacher ì¶”ë¡  (Studentì™€ ë™ì¼í•œ ë””ë°”ì´ìŠ¤ì˜ ë°ì´í„° ì‚¬ìš©)
        with torch.no_grad():
            # ìŠ¤ìŠ¹ ëª¨ë¸ì´ ì—¬ëŸ¬ GPUì— ë‚˜ë‰˜ì–´ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ inputsë¥¼ ì ì ˆíˆ ë°°ì¹˜
            teacher_outputs = teacher(**inputs)

        s_logits = outputs.logits
        t_logits = teacher_outputs.logits
        
        # KL Divergence Loss (T=2.0)
        T = 2.0
        # ìŠ¤ìŠ¹ê³¼ ì œìì˜ ì§€ì‹ ì°¨ì´ë¥¼ ê³„ì‚°
        kd_loss = nn.KLDivLoss(reduction="batchmean")(
            F.log_softmax(s_logits / T, dim=-1),
            F.softmax(t_logits / T, dim=-1)
        ) * (T ** 2)
        
        return (kd_loss, outputs) if return_outputs else kd_loss

# 5. íŠ¸ë ˆì´ë‹ ì¸ì ì„¤ì •
args = TrainingArguments(
    output_dir=OUTPUT_KD,
    max_steps=1500,               # 500ë³´ë‹¤ëŠ” í™•ì‹¤í•˜ê³  5000ë³´ë‹¤ëŠ” ë¹ ë¥¸ 'ì•ˆì „ ì§€ëŒ€'
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8, 
    learning_rate=1e-5,          
    bf16=True,
    logging_steps=10,
    save_strategy="steps",        # í˜¹ì‹œ ëª¨ë¥´ë‹ˆ ì¤‘ê°„ ì €ì¥
    save_steps=500,               # 500ìŠ¤í…ë§ˆë‹¤ ì €ì¥í•´ì„œ ë¶ˆì•ˆí•˜ë©´ ì¤‘ê°„ê±¸ë¡œë¼ë„ ì œì¶œ
    report_to="none"
)

# 6. í•™ìŠµ ì‹œì‘
print("ğŸš€ [ìµœì¢… ì‹œë„] Masked KD í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤!")
trainer = MaskedKDTrainer(
    model=student, 
    args=args, 
    train_dataset=kd_data,
    data_collator=data_collator
)

trainer.train()

# 7. ìµœì¢… ì €ì¥ ì „ ë§ˆìŠ¤í¬ ë§ˆì§€ë§‰ í™•ì¸ ë° ì €ì¥
print("âœ… í•™ìŠµ ì™„ë£Œ! ìµœì¢… ëª¨ë¸ ì €ì¥ ì¤‘...")
apply_masks(student, masks)
student.save_pretrained(OUTPUT_KD)
tokenizer.save_pretrained(OUTPUT_KD)
print(f"ğŸ‰ ëª¨ë“  ê³¼ì • ì™„ë£Œ. ëª¨ë¸ì´ {OUTPUT_KD}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
