# @title [ì¢…í•© í‰ê°€] ëª¨ë¸ë³„ ëœë¤ NíšŒ ë°˜ë³µ í‰ê·  ë¦¬ë”ë³´ë“œ
import torch
import time
import os
import gc
import random
import numpy as np
import pandas as pd
from pathlib import Path
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer
from sentence_transformers import SentenceTransformer, util

# 1. ì„¤ì •
MODEL_ID = "LGAI-EXAONE/EXAONE-4.0-1.2B"
NUM_ROUNDS = 15
QUESTIONS_PER_ROUND = 3 # ëª‡ë²ˆ ì±„ì ì‹œì¼œì„œ í‰ê°€í• ì§€

model_paths = [
    "/content/5_30down_ign_1024_Front4_model",             
    "/content/5_30down_ign_1024_Front6_model",  
    "/content/5_30down_ign_1024_Shuffle_Seq1024_model",
    "/content/5_30down_ign_2048_model",
    "/content/5_30down_ign_204_Seq256_model",
    "/content/5_30down_ign_1024_model",
    "/content/5_30down_ign_damp005_model",
    "/content/ign_model/ign_model_model"
]

device = "cuda" if torch.cuda.is_available() else "cpu"
sim_model = SentenceTransformer('all-MiniLM-L6-v2', device=device)
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)

# 2. ë°ì´í„°ì…‹ ë¡œë“œ
print("[INFO] MANTA ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
full_ds = load_dataset("LGAI-EXAONE/MANTA-1M", split="train")
total_data_size = len(full_ds)

# 3. ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”© (Ref ê¸°ì¤€ì )
print("[INFO] ë² ì´ìŠ¤ ëª¨ë¸(Ref) ë¡œë”© ì¤‘...")
model_ref = AutoModelForCausalLM.from_pretrained(
    MODEL_ID, torch_dtype=torch.float16, device_map="auto", trust_remote_code=True
)

all_history = []

# 4. ë°˜ë³µ í‰ê°€ ë£¨í”„ ì‹œì‘
for r in range(NUM_ROUNDS):
    print(f"\n" + "ğŸ”„" * 20)
    print(f"  [Round {r+1}/{NUM_ROUNDS}] ëœë¤ ìƒ˜í”Œë§ í‰ê°€ ì‹œì‘")
    print("ğŸ”„" * 20)

    # ë¼ìš´ë“œë³„ ëœë¤ ì§ˆë¬¸ ì¶”ì¶œ
    random_indices = random.sample(range(total_data_size), QUESTIONS_PER_ROUND)
    round_prompts = [full_ds[idx]['conversations'][0]['content'] for idx in random_indices]
    print(f"[INFO] ì„ íƒëœ ì¸ë±ìŠ¤: {random_indices}")

    for path in model_paths:
        if not os.path.exists(path):
            print(f"[ê²½ê³ ] ê²½ë¡œ ì—†ìŒ: {path}")
            continue

        model_name = os.path.basename(path)
        print(f"\n> í‰ê°€ ì¤‘: {model_name}")
        
        try:
            model_quant = AutoModelForCausalLM.from_pretrained(
                path, device_map="auto", trust_remote_code=True, local_files_only=True
            )
            
            p_scores = []
            s_scores = []
            
            for prompt in round_prompts:
                messages = [{"role": "user", "content": prompt}]
                input_tensor = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt").to(device)
                input_len = input_tensor.shape[1]

                # Ref ì¶”ë¡ 
                torch.cuda.synchronize()
                start = time.time()
                out_ref = model_ref.generate(input_tensor, max_new_tokens=100, do_sample=False)
                torch.cuda.synchronize()
                tps_ref = (time.time() - start) / max(len(out_ref[0][input_len:]), 1)

                # Qnt ì¶”ë¡ 
                torch.cuda.synchronize()
                start = time.time()
                out_quant = model_quant.generate(input_tensor, max_new_tokens=100, do_sample=False)
                torch.cuda.synchronize()
                tps_quant = (time.time() - start) / max(len(out_quant[0][input_len:]), 1)

                # ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚°
                ans_ref = tokenizer.decode(out_ref[0][input_len:], skip_special_tokens=True)
                ans_quant = tokenizer.decode(out_quant[0][input_len:], skip_special_tokens=True)
                
                perf = util.pytorch_cos_sim(sim_model.encode(ans_ref, convert_to_tensor=True), 
                                            sim_model.encode(ans_quant, convert_to_tensor=True)).item()
                speed = tps_ref / tps_quant
                
                p_scores.append(perf)
                s_scores.append(speed)

            # ë¼ìš´ë“œ ê²°ê³¼ ì €ì¥
            all_history.append({
                "Model": model_name,
                "Perf": np.mean(p_scores),
                "Speed": np.mean(s_scores)
            })

            # ë©”ëª¨ë¦¬ í•´ì œ
            del model_quant
            gc.collect()
            torch.cuda.empty_cache()

        except Exception as e:
            print(f"[ì—ëŸ¬] {model_name}: {e}")

# 5. ìµœì¢… í†µê³„ ì‚°ì¶œ
print("\n" + "ğŸ“Š" * 25)
print("   [ìµœì¢… ê²°ê³¼] ëª¨ë“  ë¼ìš´ë“œ í‰ê·  ë¦¬ë”ë³´ë“œ")
print("ğŸ“Š" * 25)

df_final = pd.DataFrame(all_history)
leaderboard = df_final.groupby("Model").agg({
    "Perf": "mean",
    "Speed": "mean"
}).reset_index()

leaderboard["Final_Score"] = 0.5 * leaderboard["Perf"] + 0.5 * leaderboard["Speed"]
leaderboard = leaderboard.sort_values(by="Final_Score", ascending=False)

print(leaderboard.to_markdown(index=False))
