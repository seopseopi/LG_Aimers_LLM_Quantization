import os
import torch
import shutil
from pathlib import Path
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer
from llmcompressor import oneshot
from llmcompressor.modifiers.quantization import GPTQModifier

# ==========================================
# 1. 환경 설정 (원본 그대로)
# ==========================================
MODEL_ID = "./model_kd"  # 우리가 KD로 지능 복구한 모델 경로
OUT_DIR = "./model_final_submission"
DATASET_ID = "LGAI-EXAONE/MANTA-1M"
DATASET_SPLIT = "train"

NUM_CALIBRATION_SAMPLES = 1024 
MAX_SEQUENCE_LENGTH = 512

# ==========================================
# 2. 레이어 무시 전략 (Front 6 + down_proj) - 원본 로직 100% 동일
# ==========================================
skip_indices_front = list(range(0, 6))
skip_layers_front = []
for i in skip_indices_front:
    skip_layers_front.extend([
        f"model.layers.{i}.self_attn.q_proj",
        f"model.layers.{i}.self_attn.k_proj",
        f"model.layers.{i}.self_attn.v_proj",
        f"model.layers.{i}.self_attn.o_proj",
        f"model.layers.{i}.mlp.gate_proj",
        f"model.layers.{i}.mlp.up_proj",
        f"model.layers.{i}.mlp.down_proj"
    ])

skip_indices_rest = list(range(6, 30))
skip_layers_rest_down = [f"model.layers.{i}.mlp.down_proj" for i in skip_indices_rest]

IGNORE = ["embed_tokens", "lm_head"] + skip_layers_front + skip_layers_rest_down

print(f"[INFO] Front-6 기반 원본 전략 적용 (무시 레이어: {len(IGNORE)}개)")

# ==========================================
# 3. 모델 및 데이터 로딩 (원본 리듬)
# ==========================================
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

# [원본 리듬] 슬라이싱을 통한 데이터 로드
ds = load_dataset(DATASET_ID, split=f"{DATASET_SPLIT}[:{NUM_CALIBRATION_SAMPLES}]")

def preprocess(example):
    return {
        "text": tokenizer.apply_chat_template(
            example["conversations"],
            add_generation_prompt=True,
            tokenize=False)
    }

# [원본 리듬] map을 통한 전처리
ds = ds.map(preprocess)

# ==========================================
# 4. GPTQ 양자화 (원본 세팅 100% 동일)
# ==========================================
recipe = [
    GPTQModifier(
        scheme="W4A16",
        targets=["Linear"],
        ignore=IGNORE,
        dampening_frac=0.1 
    )
]

print(f"[INFO] 5_30down_ign_1024_Front6_REPRO 양자화 시작...")

oneshot(
    model=model,
    dataset=ds,
    recipe=recipe,
    max_seq_length=MAX_SEQUENCE_LENGTH,
    num_calibration_samples=NUM_CALIBRATION_SAMPLES,
)

# ==========================================
# 5. 저장 및 압축
# ==========================================
if os.path.exists(OUT_DIR):
    shutil.rmtree(OUT_DIR)
os.makedirs(OUT_DIR, exist_ok=True)

# save_compressed=True를 통해 9분대 추론 속도 확보
model.save_pretrained(OUT_DIR, save_compressed=True)
tokenizer.save_pretrained(OUT_DIR)

# 최종 제출용 ZIP 생성
shutil.make_archive(base_name="FINAL_SUBMISSION_MODEL", format="zip", root_dir=".", base_dir=OUT_DIR)
print(f"[INFO] 최종 모델 압축 완료: FINAL_SUBMISSION_MODEL.zip")
